{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scrapping from Amazon website.\n",
        "\n",
        "## 1.1 üìå Introduction\n",
        "\n",
        "E-commerce platforms like Amazon host millions of products, generating a wealth of information that can be valuable for businesses, researchers, and consumers. Product details such as prices, reviews, and ratings change frequently, making it difficult to track trends manually. Web scraping provides a practical way to extract this information in a structured format for analysis.\n",
        "\n",
        "This project demonstrates how to build an Amazon Web Scraper using Python to automatically collect product data. By leveraging libraries like Requests, BeautifulSoup, and pandas, the scraper extracts relevant details ‚Äî including product names, prices, ratings, and reviews ‚Äî and stores them for further analysis.\n",
        "\n",
        "---\n",
        "## 1.2 üîç Problem Statement\n",
        "\n",
        "Manually tracking Amazon product data is time-consuming, inconsistent, and prone to error. For businesses, it‚Äôs crucial to monitor competitor pricing, product availability, and customer sentiment to stay competitive. Similarly, researchers and analysts need reliable datasets to study market trends and consumer preferences.\n",
        "\n",
        "## 1.3 üéØ Objectives\n",
        "\n",
        "The key objectives of this project are:\n",
        "\n",
        "\n",
        "1.   Data Extraction ‚Äì Scrape product information (titles, prices, ratings, and reviews) from Amazon product pages.\n",
        "2.   Data Cleaning & Storage ‚Äì Organize the extracted data into a structured format (CSV) for easy use.\n",
        "3.   Portfolio Demonstration ‚Äì Showcase practical web scraping and Python programming skills in a real-world use case.\n",
        "\n"
      ],
      "metadata": {
        "id": "JVC75ULKv8VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Libraries"
      ],
      "metadata": {
        "id": "rQke0zLJxjdr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XYK2WTk8huh2"
      },
      "outputs": [],
      "source": [
        " from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fetching variables before we create the dataset\n",
        "\n",
        "---\n",
        "### 3.1 Fetch product title"
      ],
      "metadata": {
        "id": "HjvrYU--9nWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Product Title\n",
        "def fetch_product_title(soup):\n",
        "  title_string = \"\" # Initialize title_string\n",
        "  try:\n",
        "    # Outer Tag Object\n",
        "    title = soup.find(\"span\", attrs={\"id\":'productTitle'}).text.strip()\n",
        "    title_string = title # Assign title to title_string\n",
        "\n",
        "  except AttributeError:\n",
        "    title_string = \"\" # Ensure title_string is defined even if there's an error\n",
        "\n",
        "  return title_string"
      ],
      "metadata": {
        "id": "7K5251njh3nG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Fetch Product Price"
      ],
      "metadata": {
        "id": "wzmgfgbF-QEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Product Price\n",
        "def fetch_product_price(soup):\n",
        "  price = \"\" # Initialize price\n",
        "  try:\n",
        "    price = soup.find(\"span\", attrs={'class':\"aok-offscreen\"}).string.strip()\n",
        "  except AttributeError: # Catching specific exception\n",
        "    price = \"\" # Ensure price is defined even if there's an error\n",
        "  return price"
      ],
      "metadata": {
        "id": "zN3wOmn2XGN3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Fetch Product Rating"
      ],
      "metadata": {
        "id": "RKkP7br7-k3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Product Rating\n",
        "def fetch_product_rating(soup):\n",
        "  rating = \"\" # Initialize rating\n",
        "  try:\n",
        "    rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
        "  except AttributeError:\n",
        "    try:\n",
        "      rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
        "    except AttributeError: # Catching specific exception\n",
        "      rating = \"\" # Ensure rating is defined even if there's an error\n",
        "  return rating"
      ],
      "metadata": {
        "id": "TX_IGinDd_xk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Fetch Number of User Reviews"
      ],
      "metadata": {
        "id": "0n8yFOor-v7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Number of User Reviews\n",
        "def fetch_product_review_count(soup):\n",
        "  review_count = \"\" # Initialize review_count\n",
        "  try:\n",
        "    review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
        "  except AttributeError:\n",
        "    review_count = \"\" # Ensure review_count is defined even if there's an error\n",
        "  return review_count"
      ],
      "metadata": {
        "id": "xESpEw8IYHab"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Fetch Availability Status"
      ],
      "metadata": {
        "id": "xx1ZQzbV-81X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract Availability Status\n",
        "def fetch_product_availability(soup):\n",
        "  available = \"Not Available\" # Initialize available\n",
        "  try:\n",
        "    available = soup.find(\"div\", attrs={'id':'availability'})\n",
        "    available = available.find(\"span\").string.strip()\n",
        "\n",
        "  except AttributeError:\n",
        "    available = \"Not Available\" # Ensure available is defined even if there's an error\n",
        "\n",
        "  return available"
      ],
      "metadata": {
        "id": "2F2uZkr4ZLrW"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dunder Main\n",
        "\n",
        "This is where we user the requests library to access the amazon website, extracting variables (gathering data on product title, product price, number of reviews, and product availability) and storing it into a dataframe which is converted to a comma-separated values (csv) file."
      ],
      "metadata": {
        "id": "lSNLngzKDb5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # add your user agent\n",
        "  HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'})\n",
        "\n",
        "  # The webpage URL\n",
        "  URL = \"https://www.amazon.com/s?k=playstation+4&ref=nb_sb_noss_2\"\n",
        "\n",
        "  # HTTP Request\n",
        "  webpage = requests.get(URL, headers=HEADERS)\n",
        "\n",
        "  # Soup Object containing all data\n",
        "  soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "\n",
        "  # Fetch links as List of Tag Objects\n",
        "  links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
        "\n",
        "  # Store the links\n",
        "  links_list = []\n",
        "\n",
        "  # Loop for extracting links from Tag Objects\n",
        "  for link in links:\n",
        "    links_list.append(link.get('href'))\n",
        "\n",
        "  d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[], \"availability\":[]}\n",
        "\n",
        "  # Loop for extracting product details from each link\n",
        "  for link in links_list:\n",
        "    new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
        "\n",
        "    page_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
        "\n",
        "    # Function calls to display all necessary product information\n",
        "    d['title'].append(fetch_product_title(page_soup))\n",
        "    d['price'].append(fetch_product_price(page_soup))\n",
        "    d['rating'].append(fetch_product_rating(page_soup))\n",
        "    d['reviews'].append(fetch_product_review_count(page_soup))\n",
        "    d['availability'].append(fetch_product_availability(page_soup))\n",
        "\n",
        "  amazon_df = pd.DataFrame.from_dict(d)\n",
        "  amazon_df['title'] = amazon_df['title'].replace('', np.nan)\n",
        "  amazon_df = amazon_df.dropna(subset=['title'])\n",
        "  amazon_df.to_csv(\"amazon_data.csv\", header=True,  index=False)"
      ],
      "metadata": {
        "id": "5OSBjOwbZoF4"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ypJ_i2IdteA"
      },
      "execution_count": 63,
      "outputs": []
    }
  ]
}